{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from pylab import rcParams\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pyltr\n",
    "\n",
    "#import data_preprocessing\n",
    "%matplotlib inline\n",
    "params = {'axes.labelsize': 14,'axes.titlesize':14, 'text.fontsize': 14, 'legend.fontsize': 14,\n",
    "          'xtick.labelsize': 13, 'ytick.labelsize': 14}\n",
    "rcParams['figure.figsize'] = 6.5, 4\n",
    "\n",
    "matplotlib.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training file\n",
    "data = pd.read_csv('C:/Users/John/Desktop/DM/Data Mining VU data/training_set_VU_DM_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training dataset - 70%\n",
    "trainset = data.head(3500)\n",
    "\n",
    "#Validation dataset - 20%\n",
    "valset = data[3501:4500]\n",
    "\n",
    "#Test dataset - 10%\n",
    "testset = data[4501:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Efi's code for data cleaning\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # removes outliers\n",
    "    # if normal, use standard deviation\n",
    "    # if not normal uses percentiles\n",
    "\n",
    "    return(df)\n",
    "\n",
    "\n",
    "def convert_type(df):\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_composite_features(df):\n",
    "    #df['date_time']= pd.to_datetime(df['date_time'])\n",
    "    #data.date_time.map(lambda x: x.month)\n",
    "\n",
    "    #df['season'] = df.date_time.apply(lambda dt: (dt.month%12 + 3)//3)\n",
    "\n",
    "\n",
    "    # Rank within the same srch id\n",
    "    #\n",
    "    df['price_rank'] = df.groupby(['srch_id'])['price_usd'].rank(method='dense')\n",
    "    df['star_rank'] = df.groupby(['srch_id'])['price_usd'].rank(method='dense')\n",
    "\n",
    "    df['value_for_money']=df.price_usd/df.prop_review_score\n",
    "    df['value_for_money'] = df.prop_review_score/df.price_usd\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def normalize_within_group(df):\n",
    "\n",
    "    # Normalize\n",
    "    df['price_usd_normalized'] = df[['price_usd','srch_id']].groupby('srch_id').transform(lambda x: (x - x.min()) / (x.max()-x.min()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def missing_values(df):\n",
    "    # continuous\n",
    "\n",
    "    # discrete\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Efi's code for data cleaning for each datasubset\n",
    "\n",
    "train = create_composite_features(trainset)\n",
    "train = normalize_within_group(trainset)\n",
    "\n",
    "val = create_composite_features(valset)\n",
    "val = normalize_within_group(valset)\n",
    "\n",
    "test = create_composite_features(testset)\n",
    "test = normalize_within_group(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Unorthodox Implementation - vectors or fearures need to be reduced/rearranged\n",
    "\n",
    "#Select the same features from each dataset\n",
    "selected_features = ['value_for_money','price_usd_normalized', 'star_rank']\n",
    "features_to_keep = ['booking_bool', 'srch_id', 'click_bool', 'prop_id']\n",
    "all_possible_features = features_to_keep + selected_features\n",
    "df_to_train = train.ix[:,all_possible_features]\n",
    "df_to_val = val.ix[:,all_possible_features]\n",
    "df_to_test = test.ix[:,all_possible_features]\n",
    "\n",
    "#Relevance = booking + clicking\n",
    "df_to_train['relevance']=df_to_train.booking_bool+df_to_train.click_bool\n",
    "df_to_val['relevance']=df_to_val.booking_bool+df_to_val.click_bool\n",
    "df_to_test['relevance']=df_to_test.booking_bool+df_to_test.click_bool\n",
    "\n",
    "#Since we've created a new variable that combined those two, there's no need in keeping them\n",
    "df_to_train.drop(['booking_bool', 'click_bool'], axis = 1)\n",
    "df_to_val.drop(['booking_bool', 'click_bool'], axis = 1)\n",
    "df_to_test.drop(['booking_bool', 'click_bool'], axis = 1)\n",
    "\n",
    "#Rearrange the columns because lambdaMART wants the target variable and the IDs in the first and second column respectively\n",
    "df_to_train = df_to_train[['relevance', 'srch_id', 'prop_id', 'value_for_money', 'price_usd_normalized', 'star_rank']]\n",
    "df_to_val = df_to_val[['relevance', 'srch_id', 'prop_id', 'value_for_money', 'price_usd_normalized', 'star_rank']]\n",
    "df_to_test = df_to_test[['relevance', 'srch_id', 'prop_id', 'value_for_money', 'price_usd_normalized', 'star_rank']]\n",
    "\n",
    "#Data cleaning\n",
    "for feature in ['value_for_money', 'price_usd_normalized', 'star_rank']:\n",
    "    df_to_train[feature][df_to_train[feature].isnull()] = df_to_train[feature].median()\n",
    "    df_to_val[feature][df_to_val[feature].isnull()] = df_to_val[feature].median()\n",
    "    df_to_test[feature][df_to_test[feature].isnull()] = df_to_test[feature].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From dataframes to arrays\n",
    "trrelevance_arr = np.array(df_to_train['relevance'])\n",
    "trfeature_arr = np.array(df_to_train[selected_features])\n",
    "trid_arr = np.array(df_to_train.srch_id)\n",
    "\n",
    "vrelevance_arr = np.array(df_to_val['relevance'])\n",
    "vfeature_arr = np.array(df_to_val[selected_features])\n",
    "vid_arr = np.array(df_to_val.srch_id)\n",
    "\n",
    "terelevance_arr = np.array(df_to_test['relevance'])\n",
    "tefeature_arr = np.array(df_to_test[selected_features])\n",
    "teid_arr = np.array(df_to_test.srch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose nDCG as metric (k arbitrary number)\n",
    "metric = pyltr.metrics.NDCG(k=10)\n",
    "\n",
    "#Use validation set, stop_after arbitrary number\n",
    "# Only needed if you want to perform validation (early stopping & trimming)\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "    vfeature_arr, vrelevance_arr, vid_arr, metric=metric, stop_after=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use lambdaMART - have to find the best values for the parametes\n",
    "\n",
    "model = pyltr.models.LambdaMART(\n",
    "    metric=metric,\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.03,\n",
    "    #max_features=0.5,\n",
    "    #query_subsample=0.5,\n",
    "    #max_leaf_nodes=10,\n",
    "    #min_samples_leaf=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(trfeature_arr, trrelevance_arr, trid_arr, monitor=monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tepred = model.predict(tefeature_arr)\n",
    "print('Random ranking:', metric.calc_mean_random(teid_arr, terelevance_arr))\n",
    "print('Our model:', metric.calc_mean(teid_arr, terelevance_arr, tepred))\n",
    "\n",
    "#Need to add Search and Property IDs to the output (easy)\n",
    "tepred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
